{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533383f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import time\n",
    "from io import StringIO\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor, Lambda, Resize, Compose, ToPILImage, Normalize, RandomCrop, RandomHorizontalFlip, RandomVerticalFlip\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e82349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22cc5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/Users/sanaali/Downloads/COMP5329S1A2Dataset 2'\n",
    "\n",
    "TRAIN_CSV = os.path.join(DIR, \"train.csv\")\n",
    "TEST_CSV = os.path.join(DIR, \"test.csv\")\n",
    "IMAGES_DIR = os.path.join(DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f7a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssignmentDataset(Dataset):\n",
    "    '''\n",
    "    The AssignmentDataset Class, child of torch.utils.data.Dataset.\n",
    "\n",
    "    Attributes:\n",
    "    csv_file (str): A string representation of the file directory to the csv data file.\n",
    "    image_dir (str): A string representation of the file directory to the images data file.\n",
    "    transform (torchvision.transforms.Compose): A torchvision.transforms.Compose object consisting of desired transforms to be made to the image data.\n",
    "    target_transform (function): A function to be passed to the label data for any desired transformations. Used to apply one-hot encoding.\n",
    "    has_labels (Bool): A boolean value to flag whether the dataset has labels or not, i.e. if it is training or testing.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, csv_file, image_dir, transform=None, target_transform=None, has_labels = True):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.has_labels = has_labels\n",
    "        \n",
    "        with open(csv_file) as file:\n",
    "            lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "            self.dataframe = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the length of the dataframe representation of the csv component of the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        self.dataframe.shape[0] (int): The length of the datasets dataframe representation.\n",
    "\n",
    "        '''\n",
    "        return self.dataframe.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Reads the data from the dataframe then outputs the observation's image tensor, OHE label tensor (if there are labels present), ImageID string and caption string in a tuple form.\n",
    "                \n",
    "        Parameters:\n",
    "        idx (int): The index for which observation to return.\n",
    "\n",
    "        Returns:\n",
    "        sample (tuple): The tuple containing the observation's image tensor, OHE label tensor (if present), ImageID string and caption string.\n",
    "        '''\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_path = os.path.join(self.image_dir, \n",
    "                                self.dataframe.iloc[idx, self.dataframe.columns.get_loc('ImageID')])\n",
    "        \n",
    "        img = io.imread(img_path)\n",
    "\n",
    "        img_id = self.dataframe.iloc[idx, self.dataframe.columns.get_loc('ImageID')]\n",
    "        caption = self.dataframe.iloc[idx, self.dataframe.columns.get_loc('Caption')]\n",
    "\n",
    "        if self.has_labels:\n",
    "          labels = self.dataframe.iloc[idx, self.dataframe.columns.get_loc('Labels')]\n",
    "          labels = labels.split(' ') # converts the string into an iterable Python list\n",
    "          labels = [int(x) for x in labels] # convert the string of numbers into integer using Pytorch for computation speed\n",
    "                     \n",
    "          if self.target_transform:\n",
    "              labels = self.target_transform(labels)\n",
    "          if self.transform:\n",
    "              img = self.transform(img)\n",
    "\n",
    "          sample = (img, labels, img_id, caption)\n",
    "          \n",
    "        else:\n",
    "          if self.transform:\n",
    "              img = self.transform(img)          \n",
    "          sample = (img, img_id, caption)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    ##Finding Mean/std\n",
    "NUM_LABELS = 19\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset_check = AssignmentDataset(csv_file = TRAIN_CSV,\n",
    "                                 image_dir = IMAGES_DIR,\n",
    "                                 transform = transforms,\n",
    "                                 target_transform = Lambda(lambda y: torch.zeros(NUM_LABELS, dtype=torch.uint8).scatter_(dim=0, index=torch.sub(torch.tensor(y), 1), value=1)),\n",
    "                                  has_labels = True,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ace925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_stats(checking_dataloader):\n",
    "  '''\n",
    "  A function to calculate the channel means and standard deviations of the image input in a training dataset.\n",
    "  Sums the channel means & squared means per image, then calculates the overall mean and standard deviation by taking the first moment (mean) and square root of the second moment (variance).\n",
    "\n",
    "  Parameters:\n",
    "  checking_dataloader (torch.utils.data.Dataloader): The Dataloader object with a batch size of 1 and containing images, image labels, image IDs and captions.\n",
    "\n",
    "  Returns:\n",
    "    means (torch.Tensor): A tensor object containing the channel means as floats. \n",
    "    stdevs (torch.Tensor): A tensor object containing the channel standard deviations as floats.\n",
    "  '''\n",
    "\n",
    "  sum_channels, sumsq_channels, n_batches = 0, 0, 0\n",
    "\n",
    "  for step, (x, _, _, _ ) in enumerate(checking_dataloader):\n",
    "    sum_channels += torch.mean(x, dim = [0, 2, 3])\n",
    "    sumsq_channels += torch.mean(x**2 , dim = [0, 2, 3])\n",
    "    n_batches += 1\n",
    "\n",
    "  means = sum_channels/n_batches\n",
    "  stdevs = (sumsq_channels/n_batches - means**2)**0.5\n",
    "\n",
    "  return means, stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b639df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61455347",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_check = DataLoader(train_dataset_check, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f159785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean is: tensor([0.4638, 0.4490, 0.4222]) \n",
      " Standard Deviation is: tensor([0.2725, 0.2698, 0.2849])\n"
     ]
    }
   ],
   "source": [
    "means, stdevs = image_stats(train_dataloader_check)\n",
    "print(f'Mean is: {means} \\n Standard Deviation is: {stdevs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c453c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_sizes(checking_dataloader):\n",
    "    '''\n",
    "    Obtains the heights and widths of the images within the supplied dataset.\n",
    "\n",
    "    Parameters:\n",
    "    checking_dataloader (torch.utils.data.Dataloader): The Dataloader object with a batch size of 1 and containing images, image labels, image IDs and captions.\n",
    "\n",
    "    Returns:\n",
    "      heights (list(int)): A list of the image heights as integers.\n",
    "      widths (list(int)): A list of the image widths as integers.\n",
    "    '''\n",
    "    \n",
    "    heights, widths = [], []\n",
    "\n",
    "    for step, (img, _, _, _) in enumerate(checking_dataloader):\n",
    "        heights.append(img.size()[2])\n",
    "        widths.append(img.size()[3])\n",
    "\n",
    "    return heights, widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594906d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median height is: 240.0\n",
      "Mean height is: 240.5502066942259\n",
      "Median width is: 320.0\n",
      "Mean width is: 289.9553940525403\n"
     ]
    }
   ],
   "source": [
    "heights, widths = get_image_sizes(train_dataloader_check)\n",
    "\n",
    "print(f'Median height is: {statistics.median(heights)}')\n",
    "print(f'Mean height is: {statistics.mean(heights)}')\n",
    "print(f'Median width is: {statistics.median(widths)}')\n",
    "print(f'Mean width is: {statistics.mean(widths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e5f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
