{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533383f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import time\n",
    "from io import StringIO\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor, Lambda, Resize, Compose, ToPILImage, Normalize, RandomCrop, RandomHorizontalFlip, RandomVerticalFlip\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e82349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22cc5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/Users/sanaali/Downloads/COMP5329S1A2Dataset 2'\n",
    "\n",
    "TRAIN_CSV = os.path.join(DIR, \"train.csv\")\n",
    "TEST_CSV = os.path.join(DIR, \"test.csv\")\n",
    "IMAGES_DIR = os.path.join(DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f7a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssignmentDataset(Dataset):\n",
    "    '''\n",
    "    The AssignmentDataset Class, child of torch.utils.data.Dataset.\n",
    "\n",
    "    Attributes:\n",
    "    csv_file (str): A string representation of the file directory to the csv data file.\n",
    "    image_dir (str): A string representation of the file directory to the images data file.\n",
    "    transform (torchvision.transforms.Compose): A torchvision.transforms.Compose object consisting of desired transforms to be made to the image data.\n",
    "    target_transform (function): A function to be passed to the label data for any desired transformations. Used to apply one-hot encoding.\n",
    "    has_labels (Bool): A boolean value to flag whether the dataset has labels or not, i.e. if it is training or testing.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, csv_file, image_dir, transform=None, target_transform=None, has_labels = True):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.has_labels = has_labels\n",
    "        \n",
    "        with open(csv_file) as file:\n",
    "            lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "            self.dataframe = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns the length of the dataframe representation of the csv component of the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        self.dataframe.shape[0] (int): The length of the datasets dataframe representation.\n",
    "\n",
    "        '''\n",
    "        return self.dataframe.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Reads the data from the dataframe then outputs the observation's image tensor, OHE label tensor (if there are labels present), ImageID string and caption string in a tuple form.\n",
    "                \n",
    "        Parameters:\n",
    "        idx (int): The index for which observation to return.\n",
    "\n",
    "        Returns:\n",
    "        sample (tuple): The tuple containing the observation's image tensor, OHE label tensor (if present), ImageID string and caption string.\n",
    "        '''\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_path = os.path.join(self.image_dir, \n",
    "                                self.dataframe.iloc[idx, self.dataframe.columns.get_loc('ImageID')])\n",
    "        \n",
    "        img = io.imread(img_path)\n",
    "\n",
    "        img_id = self.dataframe.iloc[idx, self.dataframe.columns.get_loc('ImageID')]\n",
    "        caption = self.dataframe.iloc[idx, self.dataframe.columns.get_loc('Caption')]\n",
    "\n",
    "        if self.has_labels:\n",
    "          labels = self.dataframe.iloc[idx, self.dataframe.columns.get_loc('Labels')]\n",
    "          labels = labels.split(' ') # converts the string into an iterable Python list\n",
    "          labels = [int(x) for x in labels] # convert the string of numbers into integer using Pytorch for computation speed\n",
    "                     \n",
    "          if self.target_transform:\n",
    "              labels = self.target_transform(labels)\n",
    "          if self.transform:\n",
    "              img = self.transform(img)\n",
    "\n",
    "          sample = (img, labels, img_id, caption)\n",
    "          \n",
    "        else:\n",
    "          if self.transform:\n",
    "              img = self.transform(img)          \n",
    "          sample = (img, img_id, caption)\n",
    "            \n",
    "        return sample\n",
    "    \n",
    "    ##Finding Mean/std\n",
    "NUM_LABELS = 19\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset_check = AssignmentDataset(csv_file = TRAIN_CSV,\n",
    "                                 image_dir = IMAGES_DIR,\n",
    "                                 transform = transforms,\n",
    "                                 target_transform = Lambda(lambda y: torch.zeros(NUM_LABELS, dtype=torch.uint8).scatter_(dim=0, index=torch.sub(torch.tensor(y), 1), value=1)),\n",
    "                                  has_labels = True,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ace925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_stats(checking_dataloader):\n",
    "  '''\n",
    "  A function to calculate the channel means and standard deviations of the image input in a training dataset.\n",
    "  Sums the channel means & squared means per image, then calculates the overall mean and standard deviation by taking the first moment (mean) and square root of the second moment (variance).\n",
    "\n",
    "  Parameters:\n",
    "  checking_dataloader (torch.utils.data.Dataloader): The Dataloader object with a batch size of 1 and containing images, image labels, image IDs and captions.\n",
    "\n",
    "  Returns:\n",
    "    means (torch.Tensor): A tensor object containing the channel means as floats. \n",
    "    stdevs (torch.Tensor): A tensor object containing the channel standard deviations as floats.\n",
    "  '''\n",
    "\n",
    "  sum_channels, sumsq_channels, n_batches = 0, 0, 0\n",
    "\n",
    "  for step, (x, _, _, _ ) in enumerate(checking_dataloader):\n",
    "    sum_channels += torch.mean(x, dim = [0, 2, 3])\n",
    "    sumsq_channels += torch.mean(x**2 , dim = [0, 2, 3])\n",
    "    n_batches += 1\n",
    "\n",
    "  means = sum_channels/n_batches\n",
    "  stdevs = (sumsq_channels/n_batches - means**2)**0.5\n",
    "\n",
    "  return means, stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b639df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61455347",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_check = DataLoader(train_dataset_check, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f159785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean is: tensor([0.4638, 0.4490, 0.4222]) \n",
      " Standard Deviation is: tensor([0.2725, 0.2698, 0.2849])\n"
     ]
    }
   ],
   "source": [
    "means, stdevs = image_stats(train_dataloader_check)\n",
    "print(f'Mean is: {means} \\n Standard Deviation is: {stdevs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c453c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_sizes(checking_dataloader):\n",
    "    '''\n",
    "    Obtains the heights and widths of the images within the supplied dataset.\n",
    "\n",
    "    Parameters:\n",
    "    checking_dataloader (torch.utils.data.Dataloader): The Dataloader object with a batch size of 1 and containing images, image labels, image IDs and captions.\n",
    "\n",
    "    Returns:\n",
    "      heights (list(int)): A list of the image heights as integers.\n",
    "      widths (list(int)): A list of the image widths as integers.\n",
    "    '''\n",
    "    \n",
    "    heights, widths = [], []\n",
    "\n",
    "    for step, (img, _, _, _) in enumerate(checking_dataloader):\n",
    "        heights.append(img.size()[2])\n",
    "        widths.append(img.size()[3])\n",
    "\n",
    "    return heights, widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594906d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median height is: 240.0\n",
      "Mean height is: 240.5502066942259\n",
      "Median width is: 320.0\n",
      "Mean width is: 289.9553940525403\n"
     ]
    }
   ],
   "source": [
    "heights, widths = get_image_sizes(train_dataloader_check)\n",
    "\n",
    "print(f'Median height is: {statistics.median(heights)}')\n",
    "print(f'Mean height is: {statistics.mean(heights)}')\n",
    "print(f'Median width is: {statistics.median(widths)}')\n",
    "print(f'Mean width is: {statistics.mean(widths)}')"
# Concluded 19 labels - one-hot encoded. Leaving label 12 in there as it'll just have zeros for this index
NUM_LABELS = 19
BATCH_SIZE = 50
RESIZE_SIZE = (299, 299)
CROP_SIZE = 224 # Seems to be the standard image size used (224)
TRAIN_VAL_PROP = 0.75
SEED = 5329

transforms = Compose([
    ToTensor(),
    Normalize(mean = means, std = stdevs),
    Resize(RESIZE_SIZE),
    RandomHorizontalFlip(),
    RandomVerticalFlip(),
    RandomCrop(CROP_SIZE)
])

# The main dataset available with labels
main_dataset = Dataset(csv_file = TRAIN_CSV,
                                 image_dir = IMAGES_DIR,
                                 transform = transforms,
                                 target_transform = Lambda(lambda y: torch.zeros(NUM_LABELS, dtype=torch.uint8).scatter_(dim=0, index=torch.sub(torch.tensor(y), 1), value=1)),
                                 has_labels = True
                                 )

# Need to further split it up
train_dataset, val_dataset = random_split(main_dataset, 
                                          [int(round(TRAIN_VAL_PROP * len(main_dataset))), int(round((1 - TRAIN_VAL_PROP) * len(main_dataset)))], 
                                          generator=torch.Generator().manual_seed(SEED)) # Setting seed to ensure consistency

# The final test dataset with no labels
test_dataset = Dataset(csv_file = TEST_CSV,
                                 image_dir = IMAGES_DIR,
                                transform = transforms,
                                 has_labels = False)

main_dataloader = DataLoader(main_dataset, batch_size = BATCH_SIZE, shuffle=True)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)

class MultiLabelDataset(Dataset):
    def __init__(self, csv_file, image_dir, transform=None, target_transform=None, has_labels=True):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            image_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied on a sample.
            target_transform (callable, optional): Transform to be applied to the labels.
            has_labels (bool): Indicator if labels are present in the CSV.
        """
        self.image_dir = image_dir
        self.transform = transform
        self.target_transform = target_transform
        self.has_labels = has_labels
        self.dataframe = pd.read_csv(csv_file)

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx]['ImageID'])
        image = io.imread(img_name)
        img_id = self.dataframe.iloc[idx]['ImageID']
        caption = self.dataframe.iloc[idx]['Caption']
        
        if self.has_labels:
            labels = list(map(int, self.dataframe.iloc[idx]['Labels'].split()))
            if self.target_transform:
                labels = self.target_transform(labels)
        else:
            labels = None

        if self.transform:
            image = self.transform(image)

        return {'image': image, 'labels': labels, 'img_id': img_id, 'caption': caption}


transforms = Compose([
    ToTensor(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), 
    Resize((299, 299)),
    RandomHorizontalFlip(),
    RandomVerticalFlip(),
    RandomCrop(224)
])
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Using Adam optimizer


def initialize_model(num_labels):
    model = models.resnet50(pretrained=True)  
    num_features = model.fc.in_features 

    model.fc = nn.Sequential(
        nn.Linear(num_features, num_labels),
        nn.Sigmoid()  
    )
    
    return model


num_labels = 19 
model = initialize_model(num_labels)


def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=25):
    for epoch in range(num_epochs):
        model.train()  
        
        running_loss = 0.0
        for inputs, labels, _, _ in train_dataloader:  # Adjust according to your dataloader
            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels.float())  # Ensure labels are float for BCELoss
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)

        epoch_loss = running_loss / len(train_dataloader.dataset)
        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')

        # Validation phase
        model.eval()  # Set model to evaluate mode
        validate_model(model, val_dataloader)

def validate_model(model, dataloader):
    total_preds, total_labels = [], []
    
    with torch.no_grad():
        for inputs, labels, _, _ in dataloader:  
            outputs = model(inputs)
            predicted_labels = (outputs > 0.5).float()  # Threshold predictions
            total_preds.append(predicted_labels)
            total_labels.append(labels)

    # Calculate F1 Score
    f1_score = compute_f1_score(torch.cat(total_labels, dim=0), torch.cat(total_preds, dim=0))
    print(f'Validation F1 Score: {f1_score:.4f}')

def compute_f1_score(labels, predictions):
    tp = (labels * predictions).sum().to(torch.float32)
    tn = ((1 - labels) * (1 - predictions)).sum().to(torch.float32)
    fp = ((1 - labels) * predictions).sum().to(torch.float32)
    fn = (labels * (1 - predictions)).sum().to(torch.float32)

    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1 = 2* (precision * recall) / (precision + recall)
    return f1.item()


train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)


train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=10)

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e5f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
